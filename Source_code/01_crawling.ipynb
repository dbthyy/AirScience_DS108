{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a882c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0deb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(places):\n",
    "    \"\"\"\n",
    "    Hàm tạo danh sách URL dựa trên danh sách địa điểm và thời gian.\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    start_day = datetime.datetime.today() + datetime.timedelta(days=1)\n",
    "    end_day = datetime.datetime.today().replace(\n",
    "        hour=0, minute=0, second=0, microsecond=0\n",
    "    ) + datetime.timedelta(days=20)\n",
    "\n",
    "    for i in range((end_day - start_day).days + 1):\n",
    "        day = (start_day + datetime.timedelta(days=i)).strftime(\"%d-%m-%Y\")\n",
    "        dates.append(day)\n",
    "\n",
    "    urls = []\n",
    "    for place in places:\n",
    "        for day in dates:\n",
    "            url = (\n",
    "                f\"https://www.traveloka.com/vi-vn/flight/fullsearch\"\n",
    "                f\"?ap={place}&dt={day}.NA&ps=1.0.0&sc=ECONOMY\"\n",
    "            )\n",
    "            urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c60a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_planetrip(url_list):\n",
    "    \"\"\"\n",
    "    Hàm crawl dữ liệu chuyến bay theo từng URL.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Edge()\n",
    "    df_by_url = {}\n",
    "\n",
    "    for url in url_list:\n",
    "        driver.get(url)\n",
    "        sleep(5)\n",
    "        wait = WebDriverWait(driver, 50)\n",
    "\n",
    "        columns = [\n",
    "            \"brand\", \"price\", \"start_time\", \"start_day\", \"end_day\", \"end_time\",\n",
    "            \"trip_time\", \"take_place\", \"destination\", \"hand_luggage\",\n",
    "            \"checked_baggage\", \"crawl_date\"\n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        brand, price, start_day, end_day = [], [], [], []\n",
    "        start_time, end_time, trip_time = [], [], []\n",
    "        take_place, destination = [], []\n",
    "        hand_luggage, checked_baggage = [], []\n",
    "        crawl_date = [datetime.datetime.today().strftime(\"%d-%m-%Y\")] * 1000\n",
    "\n",
    "        # Cuộn trang để load toàn bộ dữ liệu\n",
    "        initial_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_step = initial_height // 200\n",
    "        for i in range(1000000):\n",
    "            position = scroll_step * (i + 1)\n",
    "            driver.execute_script(f\"window.scrollTo(0, {position});\")\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if position >= new_height:\n",
    "                break\n",
    "        driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "\n",
    "        elements = driver.find_elements(\n",
    "            By.XPATH,\n",
    "            \"//div[@class='css-1dbjc4n r-9nbb9w r-otx420 \"\n",
    "            \"r-1i1ao36 r-1x4r79x']\"\n",
    "        )\n",
    "\n",
    "        price_elements = wait.until(EC.visibility_of_all_elements_located((\n",
    "            By.XPATH,\n",
    "            \"//div[@class='css-1dbjc4n r-obd0qt r-eqz5dr \"\n",
    "            \"r-9aw3ui r-knv0ih r-ggk5by']//h3\"\n",
    "        )))\n",
    "\n",
    "        brand_elements = wait.until(EC.visibility_of_all_elements_located((\n",
    "            By.XPATH,\n",
    "            \"//div[@class='css-1dbjc4n r-1habvwh r-18u37iz r-1ssbvtb']//div\"\n",
    "        )))\n",
    "\n",
    "        detail_elements = wait.until(EC.visibility_of_all_elements_located((\n",
    "            By.XPATH,\n",
    "            \"//div[@class='css-1dbjc4n r-13awgt0 r-18u37iz \"\n",
    "            \"r-f4gmv6 r-1777fci']\"\n",
    "        )))\n",
    "\n",
    "        old_elements = []\n",
    "        for i, detail in enumerate(detail_elements):\n",
    "            if elements[i] in old_elements:\n",
    "                continue\n",
    "            ActionChains(driver).move_to_element(detail).click().perform()\n",
    "            old_elements.append(elements[i])\n",
    "\n",
    "            brand.append(brand_elements[i].text)\n",
    "            price.append(price_elements[i].text)\n",
    "\n",
    "            start_time.append(wait.until(EC.visibility_of_element_located((\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class,'r-e8mqni') and contains(@class,'ttb5dx')]\"\n",
    "                \"//div[contains(@class,'r-5oul0u')]\"\n",
    "            ))).text)\n",
    "\n",
    "            end_time.append(wait.until(EC.visibility_of_element_located((\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class,'r-q3we1') and contains(@class,'ttb5dx')]\"\n",
    "                \"//div[contains(@class,'r-fdjqy7')]\"\n",
    "            ))).text)\n",
    "\n",
    "            start_day.append(wait.until(EC.visibility_of_element_located((\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class,'r-e8mqni') and contains(@class,'ttb5dx')]\"\n",
    "                \"//div[contains(@class,'r-fdjqy7')]\"\n",
    "            ))).text)\n",
    "\n",
    "            end_day.append(wait.until(EC.visibility_of_element_located((\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class,'r-q3we1') and contains(@class,'ttb5dx')]\"\n",
    "                \"//div[contains(@class,'r-fdjqy7')]\"\n",
    "            ))).text)\n",
    "\n",
    "            trip_time.append(wait.until(EC.visibility_of_element_located((\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class,'r-13awgt0') and contains(@class,'r-fdjqy7')]\"\n",
    "            ))).text)\n",
    "\n",
    "            destination.append(wait.until(EC.visibility_of_element_located((\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class,'r-e8mqni') and contains(@class,'r-q3we1')]\"\n",
    "            ))).text)\n",
    "\n",
    "            take_place.append(wait.until(EC.visibility_of_element_located((\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class,'r-e8mqni') and not(contains(@class,'r-q3we1'))]\"\n",
    "            ))).text)\n",
    "\n",
    "            baggage_elements = wait.until(EC.presence_of_all_elements_located((\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class,'r-19u6a5r')]\"\n",
    "            )))\n",
    "            temp_hand, temp_checked = \"\", \"\"\n",
    "            for baggage in baggage_elements:\n",
    "                text = baggage.text.lower()\n",
    "                if \"xách tay\" in text:\n",
    "                    temp_hand = baggage.text\n",
    "                elif \"hành lý\" in text:\n",
    "                    temp_checked = baggage.text\n",
    "            hand_luggage.append(temp_hand)\n",
    "            checked_baggage.append(temp_checked)\n",
    "\n",
    "            new_df = pd.DataFrame(list(zip(\n",
    "                brand, price, start_time, start_day, end_time, end_day,\n",
    "                trip_time, take_place, destination, checked_baggage,\n",
    "                hand_luggage, crawl_date\n",
    "            )), columns=columns)\n",
    "\n",
    "            df = pd.concat([df, new_df], axis=0, ignore_index=True)\n",
    "\n",
    "            brand, price, start_day, end_day = [], [], [], []\n",
    "            start_time, end_time, trip_time = [], [], []\n",
    "            take_place, destination = [], []\n",
    "            hand_luggage, checked_baggage = [], []\n",
    "            crawl_date = [datetime.datetime.now().strftime(\"%d-%m-%y\")] * 1000\n",
    "\n",
    "            detail.click()\n",
    "            sleep(5)\n",
    "\n",
    "        new_url = url[53:]\n",
    "        df.to_csv(f\"planetrip_{new_url}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        df_by_url[new_url] = df\n",
    "\n",
    "    driver.quit()\n",
    "    return df_by_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_list = [\n",
    "        \"SGN.PQC\", \"SGN.HAN\", \"SGN.DAD\",\n",
    "        \"SGN.HPH\", \"SGN.DLI\", \"SGN.CXR\"\n",
    "]\n",
    "\n",
    "for des in des_list:\n",
    "    print(f\"Destination: {des}\")\n",
    "    url_list = get_url([des]) \n",
    "    df_by_url = crawl_planetrip(url_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
